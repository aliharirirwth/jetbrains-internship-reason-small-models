# Phase 2 — Recurrent reasoning (Sudoku, depth vs accuracy)

We evaluate **small models (BERT-Small and TRM) in a recurrent loop** on 4×4 Sudoku: at each step the model sees the current grid and predicts one cell update; **state is passed to the next step**. The goal is to analyse **how effective compute depth (number of steps) influences reasoning accuracy** under a fixed per-step cost, per the project brief.

## Design

- **Task:** 4×4 Sudoku (digits 1–4; 2×2 blocks). Puzzles are generated by solving a random grid and punching holes.
- **Models (all share the same recurrent-loop interface):**
  - **BERT-Small (`SudokuBERT`):** Grid encoded as a sequence of 16 “tokens” (one per cell); value + position embeddings, stacked transformer layers, per-cell head → 16×4 logits. BERT-like encoder embedded in the loop.
  - **TRM (`SudokuTRM`):** Same sequence embedding; one transformer block applied **recurrently** K times (internal recurrence), then per-cell head. Transformer-based recurrent model as in the brief.
  - **MLP:** Baseline feedforward predictor (80 → 128 → 64 → 16×4), no attention.
- **State:** Current grid encoded as 80 features (one-hot per cell: empty + digits 1–4). **State is passed between steps** as the updated grid after each predicted move.
- **Step:** Model outputs 16×4 logits (per-cell, per-digit). We pick the best valid (cell, digit) and fill that cell. The updated grid is the state for the next step.
- **Metric:** Cell accuracy over initially empty cells after K steps.

## Quick start

```bash
# From repo root
pip install torch numpy matplotlib
python -m reasoning.run_depth_experiment --out_dir reasoning/figures
```

By default this runs **all three models** (MLP, BERT, TRM), trains each on 200 puzzles (25 epochs), then runs each recurrently for depths 1, 2, 4, 8, 16 and saves:

- `reasoning/figures/depth_vs_accuracy.png` — comparison plot (depth vs accuracy for MLP, BERT-Small, TRM)
- `reasoning/figures/depth_vs_accuracy.json` — same data in JSON

To run a single model:

```bash
python -m reasoning.run_depth_experiment --model bert --out_dir reasoning/figures
python -m reasoning.run_depth_experiment --model trm --out_dir reasoning/figures
```

## Options

- `--model mlp|bert|trm|all`: Model to train and evaluate (default: `all` for BERT vs TRM vs MLP comparison).
- `--no_train`: Skip training and evaluate a random model (quick run; accuracy will be low).
- `--epochs 50`: Train longer.
- `--depths "1,2,4,8,16,32"`: Depths to evaluate.
- `--train_size 500 --val_size 200`: More data.

## Interpretation

- **Depth = 1:** One step of the model (one cell filled).
- **Depth = K:** K steps; the same model is applied K times, each time seeing the updated grid. More steps → more “compute depth” and typically better accuracy until the puzzle is solved or the model stops changing the grid.

The figure shows that **increasing recurrent depth improves reasoning accuracy**, illustrating how recurrence and compute depth help small models on a structured task.
